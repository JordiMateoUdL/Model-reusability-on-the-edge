{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "193fa404",
   "metadata": {},
   "source": [
    "# Experimental Setup\n",
    "\n",
    "This notebook will use the data obtained from [GNFUV Unmanned Surface Vehicles Sensor Data Set 2 Data Set](https://archive.ics.uci.edu/ml/datasets/GNFUV+Unmanned+Surface+Vehicles+Sensor+Data+Set+2). This data set contains eight (2x4) data sets of mobile sensor readings data (humidity, temperature) corresponding to a swarm of four Unmanned Surface Vehicles (USVs) in a test-bed, Athens, Greece.\n",
    "\n",
    "**Dataset characteristics**: Multivariate and Sequential \n",
    "\n",
    "**Attributes**:\n",
    "* 'device' = USV ID (String)\n",
    "* 'humidity' = sensed humidity value from the USV sensor (real value)\n",
    "* temperature' = sensed temperature value from the USV sensor (real value)\n",
    "* 'experiment' = 1 (constant real value)\n",
    "* 'time' = the sensing and reporting time (real value)\n",
    "* 'pi' = Raspberry Pi ID\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to control if activate all the verbose will be plotted.\n",
    "debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from json import JSONDecodeError\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_dataset(home_dir,dataset_name):\n",
    "    \n",
    "    \"\"\"generate_dataset: This function assumes the data is \n",
    "    distributed in folders from a parent directory. \n",
    "    Each folder contains comma-separated files where each \n",
    "    row is defined with a JSON notation.\n",
    "    Arguments:\n",
    "        home_dir [string] -- parent directory\n",
    "        dataset_name [ [string] ] -- Target folder name\n",
    "    Returns:\n",
    "        {data_df} -- Pandas dataframe with all the information joined,\n",
    "                    cleaned and ready to use.\n",
    "    \"\"\"\n",
    "    home_dir = home_dir +\"/\" + dataset_name\n",
    "    pi_dirs = os.listdir(home_dir)\n",
    "    data_list = []\n",
    "    columns = None\n",
    "    \n",
    "    for pi_dir in pi_dirs:\n",
    "        if 'pi' not in pi_dir:\n",
    "            continue\n",
    "        curr_dir = os.path.join(home_dir, pi_dir)\n",
    "        data_file = os.path.join(curr_dir, os.listdir(curr_dir)[0])\n",
    "        with open(data_file, 'r') as f:\n",
    "            line = f.readline().strip().replace(\"'\", '\"')\n",
    "            while line != '':\n",
    "                try:\n",
    "                    input_json = json.loads(line)\n",
    "                    sensor_datetime = datetime.fromtimestamp(input_json['time'])\n",
    "                    input_json['time'] = sensor_datetime\n",
    "                    input_json['pi'] = pi_dir\n",
    "                    data_list.append(list(input_json.values()))\n",
    "                    if columns is None:\n",
    "                        columns = list(input_json.keys())\n",
    "                except JSONDecodeError as je:\n",
    "                    pass\n",
    "                line = f.readline().strip().replace(\"'\", '\"')\n",
    "    \n",
    "    data_df = pd.DataFrame(data_list, columns=columns)\n",
    "    \n",
    "    #@TODO: Include an argument to decide the target columns we want in the final dataset.\n",
    "    del data_df['experiment']\n",
    "    del data_df['device']\n",
    "    del data_df['time']\n",
    "    del data_df['pi']\n",
    "    data_df = data_df.replace(to_replace=' None', value=np.nan).dropna()\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d46e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_dir = '../datasets'\n",
    "dataset = 'GNFUV-USV-Dataset'\n",
    "\n",
    "# This variable contains the name of the columns and represents the variables we want to study\n",
    "target_columns=['temperature','humidity']\n",
    "\n",
    "# D represents the global dataset (all the points).\n",
    "D = generate_dataset(home_dir,dataset)\n",
    "\n",
    "if debug:\n",
    "    for t in target_columns:\n",
    "        print(D[t].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21b9be7",
   "metadata": {},
   "source": [
    "### Distribution of D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Plot the distribution (2D)\n",
    "sns.pairplot(D, height=2.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicies to access row values\n",
    "Xtrain=0\n",
    "Xtest=1\n",
    "Ytrain=2\n",
    "Ytest=3\n",
    "\n",
    "def split_data(x,y, test_size=0.2):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=test_size,random_state=42)\n",
    "    return [x_train,x_test,y_train,y_test]\n",
    "\n",
    "def plot_SVR(tx,ty,py,name='', x_name='humidity',y_name='temperature'):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if debug:\n",
    "        print(\"X (queries): \" + str(tx))\n",
    "        print(\"Y (expected): \" + str(ty))\n",
    "        print(\"Y (predicted): \" + str(py))\n",
    "    \n",
    "    plt.scatter(tx, ty, c='tab:green', label='original')\n",
    "    plt.plot(tx, py, c='tab:red', label='predicted')\n",
    "    \n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(y_name)\n",
    "    plt.title('Support Vector Regression ' +str(name))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"MSE:\", mean_squared_error(ty, py))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b551a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _D represents [x_train,x_test,y_train,y_test] with data from D\n",
    "X=D.copy().drop(columns=[\"temperature\"])\n",
    "y=D[\"temperature\"].copy()\n",
    "_D = split_data(X,y,test_size=0.2)\n",
    "#_D = split_data(D,target_columns[1],target_columns[0],test_size=0.2)\n",
    "\n",
    "# M represent the big brother model with a full vision of all the data\n",
    "from sklearn.svm import SVR\n",
    "model = SVR(kernel='rbf', C=1, gamma=0.1, epsilon=0.1)\n",
    "\n",
    "# For this experminet X=represent humidity and y=temperature\n",
    "X=_D[Xtrain].values.reshape(-1, 1)\n",
    "y=_D[Ytrain].values\n",
    "\n",
    "M=model.fit(X, y)\n",
    "\n",
    "x = _D[Xtest].values.reshape(-1, 1)\n",
    "py = M.predict(x)\n",
    "\n",
    "plot_SVR(x,_D[Ytest],py,x_name='humidity',y_name='temperature')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4311f1",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "This section aims to prepare the dataset to work with. The idea is to split the dataset into different groups to represent a node in our edge layer.\n",
    "\n",
    "So, we want to represent our edge layer as a set of nodes ($n \\in N$).\n",
    "Each node $n$ in the edge layer will contain a unique subset of points from the full dataset $D$. Therefore, $D_{n}\\subset D \\;,\\; \\forall n \\in N\\;|\\; D_n\\not\\subset D_m\\;, \\; \\forall m \\in N \\;, \\;m \\neq n$.\n",
    "\n",
    "This way, we use **KMeans** to generate different clusters and assign each cluster to a different node. *Note*: $K=N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for generating classification, regression and clustering datasets\n",
    "import sklearn.datasets as dt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def generate_data(D,K,x_name,y_name):\n",
    "    from pandas import DataFrame\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    kmeans = KMeans(n_clusters=K).fit(D)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # We build a dictionary for each cluster with the corresponding dataframe using the position obtained in Kmeans\n",
    "    # We split the x,y into train and test sets\n",
    "    \n",
    "    D_ = {}\n",
    "    \n",
    "    samples = {kmeans.cluster_centers_[i, 0]: np.where(kmeans.labels_ == i)[0] for i in range(kmeans.n_clusters)}\n",
    " \n",
    "    for k in samples.keys():\n",
    "        D_[k]={}\n",
    "        d = D.iloc[samples[k], :]\n",
    "        D_[k][\"full\"]=d\n",
    "        X=d.copy().drop(columns=[\"temperature\"])\n",
    "        y=d[\"temperature\"].copy()\n",
    "        D_[k][\"split\"]=split_data(X,y,test_size=0.2)\n",
    "        \n",
    "    return centroids, D_\n",
    "    \n",
    "\n",
    "def plot_nodes(D,centroids,x_name,y_name):\n",
    "    labels = ['Node{0}'.format(i) for i in range(len(centroids))]\n",
    "    plt_data = plt.scatter(D[x_name], D[y_name], c= kmeans.labels_.astype(float))\n",
    "    plt.colorbar()\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], marker='x')\n",
    "    plt.title('K-means clustering')\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(y_name)\n",
    "    labels = ['Node{0}'.format(i) for i in range(K)]\n",
    "    for i in range (K):\n",
    "        xy=(centroids[i, 0],centroids[i, 1])\n",
    "        plt.annotate(labels[i],xy, horizontalalignment='right', verticalalignment='top',\n",
    "              bbox=dict(boxstyle='round,pad=0.2', fc='yellow', alpha=0.3),\n",
    "              arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.95', \n",
    "                                color='b'))\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c439c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=10\n",
    "centroids,D_ = generate_data(D,K,target_columns[1],target_columns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c306e04",
   "metadata": {},
   "source": [
    "Let us check the different distributions of each node concerning the two variables studied, humidity and temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for k in D_.keys():\n",
    "    g = sns.pairplot(D_[k][\"full\"], height=2.5);\n",
    "    g.fig.suptitle(\"Node \"+str(i)) \n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804893ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "m={}\n",
    "for k in D_.keys():\n",
    "    # For this experminet X=represent humidity and y=temperature\n",
    "    X = D_[k][\"split\"][Xtrain].values.reshape(-1, 1)\n",
    "    y = D_[k][\"split\"][Ytrain].values\n",
    "    m[k] =  model.fit(X,y)\n",
    "    x = D_[k][\"split\"][Xtest].values.reshape(-1, 1)\n",
    "    py = m[k].predict(x)\n",
    "    plot_SVR(x,D_[k][\"split\"][Ytest].values,py,x_name='humidity',y_name='temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b659d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outsiders(input_Ds, strategy, outer):\n",
    "    output_Ds={}\n",
    "    if (strategy == 'percentile' or strategy == 'random'):\n",
    "        percentile = ((100/outer)/100) - 0.01\n",
    "        for k in input_Ds.keys():\n",
    "            output_Ds[k]=input_Ds[k].copy()\n",
    "            for k1 in input_Ds.keys():\n",
    "                if (k != k1):\n",
    "\n",
    "                    for o in range(outer):\n",
    "                        dk1 = output_Ds[k][\"full\"].copy()\n",
    "            \n",
    "                        if (strategy == 'percentile'):\n",
    "                            v =  percentile + (percentile*o)\n",
    "                            vDf=pd.DataFrame(dk1.quantile(v)).T\n",
    "                        elif (strategy == 'random'):\n",
    "                            vDf = dk1.sample()\n",
    "                            \n",
    "                        dfX=output_Ds[k][\"split\"][Xtrain]\n",
    "                        dfY=output_Ds[k][\"split\"][Ytrain]\n",
    "                        \n",
    "                        dfX.loc[len(dfX.index)] = vDf.iloc[:,1].values[0]\n",
    "                        dfY.loc[len(dfY.index)] = vDf.iloc[:,0].values[0]\n",
    "                                                              \n",
    "    else:\n",
    "        raise ValueError(\"Wrong strategy. Strategy must be ['random','percentile']\")\n",
    "             \n",
    "    return output_Ds\n",
    "\n",
    "def get_MSE(model,y_expected,x):\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    y_pred = model.predict(x)\n",
    "    return mean_squared_error(y_expected.reshape(-1, 1), y_pred)\n",
    "\n",
    "\n",
    "# Sensitivity analysis to evaluate the impact of the size in the outer points set.\n",
    "def do_sa_outer(outersList,ds,model,M):\n",
    "    experiments={}\n",
    "    for outer in outersList:\n",
    "    \n",
    "    # Step 1: Generate the outsiders using the outer value at each iteration\n",
    "    \n",
    "        # Step 1.1: Using the percentile strategy\n",
    "        nodes_dataset_with_outsiders = generate_outsiders(ds,'percentile',outer)\n",
    "        # Step 1.2: Using the random strategy\n",
    "        nodes_dataset_with_outsiders_random = generate_outsiders(ds,'random',outer)\n",
    "\n",
    "        nodes_model={}\n",
    "        experiments[outer] = {}\n",
    "\n",
    "        # Step 2. We want to evaluate all the testdata, so we perform a test at each node \n",
    "        for k in ds.keys():\n",
    "            experiments[outer][k] = {}\n",
    "            # Step 2.1. Build test data from node k\n",
    "            # Step 2.1.1.  Test data from node k (variable x)\n",
    "            x = ds[k][\"split\"][Xtest].values.reshape(-1, 1) \n",
    "            # Step 2.1.2. Expected data from node k (variable y)\n",
    "            y_expected=ds[k][\"split\"][Ytest].values\n",
    "            \n",
    "            # Step 2.2. Train model without outsiders (only their data)\n",
    "            for k1 in ds.keys():\n",
    "                 X = ds[k][\"split\"][Xtrain].values.reshape(-1, 1)\n",
    "                 y = ds[k][\"split\"][Ytrain].values\n",
    " \n",
    "                 nodes_model[k1] =  model.fit(X,y)\n",
    "\n",
    "            # Step 3: Make predictions\n",
    "\n",
    "            # 3.1 Big brother model\n",
    "            experiments[outer][k]['full']=get_MSE(M,y_expected,x)\n",
    "\n",
    "            experiments[outer][k]['percentile']=[]\n",
    "            experiments[outer][k]['random']=[]\n",
    "            experiments[outer][k]['node']=[]\n",
    "            nodes_with_outsiders_model={}\n",
    "            nodes_with_outsiders_random={}\n",
    "\n",
    "            # 3.2 Models in the nodes\n",
    "            for k1 in ds.keys():\n",
    "                X = nodes_dataset_with_outsiders[k1][\"split\"][Xtrain].values.reshape(-1, 1)\n",
    "                y = nodes_dataset_with_outsiders[k1][\"split\"][Ytrain].values\n",
    "                nodes_with_outsiders_model[k1] =  model.fit(X,y)\n",
    "                X = nodes_dataset_with_outsiders_random[k1][\"split\"][Xtrain].values.reshape(-1, 1)\n",
    "                y = nodes_dataset_with_outsiders_random[k1][\"split\"][Ytrain].values\n",
    "                nodes_with_outsiders_random[k1] = model.fit(X,y)\n",
    "\n",
    "                # 3.3 using node data\n",
    "                experiments[outer][k]['node'].append(get_MSE(nodes_model[k1],y_expected,x))\n",
    "\n",
    "                # 3.1 percentile strategy\n",
    "                experiments[outer][k]['percentile'].append(get_MSE(nodes_with_outsiders_model[k1],y_expected,x))\n",
    "\n",
    "                # 3.2 random strategy\n",
    "                experiments[outer][k]['random'].append(get_MSE(nodes_with_outsiders_random[k1],y_expected,x))\n",
    "\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d74f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "outersList = [5,7,10,15,17,20,25,30]\n",
    "experiment1=do_sa_outer(outersList,D_,model,M)\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(experiment1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0539a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
